# ğŸ¤– Production-Grade GenAI Assistant with RAG

A production-ready chat assistant powered by Google Gemini embeddings, implementing Retrieval-Augmented Generation (RAG) for grounded, context-aware responses with SQLite persistence.

## âœ… System Status: FULLY OPERATIONAL

- Backend: Node.js + Express âœ…
- Database: SQLite âœ…  
- Embeddings: gemini-embedding-001 âœ…
- Vector Store: 10 chunks loaded âœ…
- RAG Pipeline: Complete âœ…
- Session Management: SQLite âœ…

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚  (Frontend) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP Request
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Express Server              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Query Embedding           â”‚  â”‚
â”‚  â”‚     (Gemini embedding-001)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. Similarity Search         â”‚  â”‚
â”‚  â”‚     (Cosine Similarity)       â”‚  â”‚
â”‚  â”‚     Vector Store (JSON)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. Context Retrieval         â”‚  â”‚
â”‚  â”‚     (Top 3 chunks, >0.7)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. LLM Generation            â”‚  â”‚
â”‚  â”‚     (Gemini 1.5 Flash)        â”‚  â”‚
â”‚  â”‚     Grounded Prompt           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
         JSON Response
```

## ğŸ“ Project Structure

```
rag-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ docs.json           # Raw knowledge base (10 documents)
â”‚   â”‚   â””â”€â”€ vector_store.json   # Processed embeddings (generated)
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ ingest.js           # Document ingestion & embedding generation
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ chunking.js         # Text chunking with overlap
â”‚   â”‚   â””â”€â”€ vector_math.js      # Cosine similarity implementation
â”‚   â””â”€â”€ server.js               # Express API server
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html              # Chat UI
â”‚   â”œâ”€â”€ styles.css              # Styling
â”‚   â””â”€â”€ app.js                  # Frontend logic
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ package.json                # Dependencies
â””â”€â”€ README.md                   # Documentation
```

## ğŸ”„ RAG Workflow

1. **Document Ingestion**
   - Load documents from `docs.json`
   - Chunk content (300-500 tokens)
   - Generate embeddings using `text-embedding-3-small`
   - Store in vector database

2. **Query Processing**
   - User submits question
   - Generate query embedding
   - Calculate cosine similarity with all chunks
   - Retrieve top 3 most relevant chunks

3. **Response Generation**
   - Apply similarity threshold (0.7)
   - Inject retrieved context into prompt
   - Include conversation history (last 5 pairs)
   - Generate response with GPT-3.5 (temp: 0.2)

4. **Grounding Strategy**
   - If similarity < threshold â†’ fallback response
   - System prompt enforces context-only answers
   - Low temperature prevents hallucination

## ğŸ“Š Embedding Strategy

- **Model**: `gemini-embedding-001` (768 dimensions)
- **Chunking**: 300 words per chunk with 50-word overlap
- **Overlap Rationale**: Maintains context across chunk boundaries
- **Similarity**: Cosine similarity (dot product / magnitude)
- **Threshold**: 0.7 (filters irrelevant results)
- **Retrieval**: Top-K (K=3)

## ğŸ¯ Similarity Search

### Cosine Similarity Formula
```javascript
similarity = (A Â· B) / (||A|| Ã— ||B||)

Where:
- A Â· B = dot product of vectors A and B
- ||A|| = magnitude (length) of vector A
- ||B|| = magnitude (length) of vector B
```

### Implementation
```javascript
function calculateCosineSimilarity(vectorA, vectorB) {
  // Dot product
  const dotProduct = vectorA.reduce((sum, val, i) => sum + val * vectorB[i], 0);
  
  // Magnitudes
  const magnitudeA = Math.sqrt(vectorA.reduce((sum, val) => sum + val * val, 0));
  const magnitudeB = Math.sqrt(vectorB.reduce((sum, val) => sum + val * val, 0));
  
  // Cosine similarity
  return dotProduct / (magnitudeA * magnitudeB);
}
```

### Retrieval Process
1. Convert user query to embedding vector
2. Calculate cosine similarity with all document chunks
3. Filter results with similarity >= 0.7
4. Sort by highest similarity
5. Return top 3 chunks

## ğŸ’¬ Prompt Design

### Grounded Prompt Template
```
System: You are a helpful support assistant. Answer the question using 
ONLY the context provided below.

If the context doesn't contain enough information, say "I don't have 
enough information to answer that question based on the available 
documentation."

CONTEXT:
1. [Retrieved Chunk 1]
2. [Retrieved Chunk 2]
3. [Retrieved Chunk 3]

[Conversation History - Last 5 pairs]

USER QUESTION: [Current Question]

ANSWER:
```

**Design Rationale**:
- **Explicit grounding**: "ONLY the context provided" prevents hallucination
- **Fallback instruction**: Handles low-confidence scenarios gracefully
- **Numbered context**: Makes it clear what information is available
- **Low temperature (0.2)**: Ensures deterministic, factual responses
- **History inclusion**: Maintains conversation flow and context

## ğŸš€ Setup Instructions

### Prerequisites
- Node.js v18+
- Gemini API key from https://aistudio.google.com/app/apikey

### Installation

1. **Install Dependencies**
```bash
npm install
```

2. **Configure Environment**
```bash
cp .env.example .env
# Edit .env and add your Gemini API key
```

3. **Generate Vector Store** (Pre-processing step)
```bash
npm run ingest
```
This will:
- Read documents from `backend/data/docs.json`
- Split them into chunks (300 words with 50-word overlap)
- Generate embeddings for each chunk
- Save to `backend/data/vector_store.json`

4. **Start Server**
```bash
npm start
```

5. **Access Application**
```
http://localhost:3000
```

## ğŸ“¡ API Documentation

### POST /api/chat

**Request**:
```json
{
  "sessionId": "session_123",
  "message": "How can I reset my password?"
}
```

**Response**:
```json
{
  "reply": "Users can reset their password from Settings > Security...",
  "tokensUsed": 120,
  "retrievedChunks": 3,
  "maxSimilarity": "0.856",
  "chunkDetails": [
    {"title": "Reset Password", "score": "0.856"}
  ]
}
```

### GET /api/health

**Response**:
```json
{
  "status": "ok",
  "vectorStoreSize": 10,
  "activeSessions": 5,
  "embeddingModel": "gemini-embedding-001",
  "chatModel": "gemini-1.5-flash",
  "database": "SQLite"
}
```

### GET /api/sessions

**Response**:
```json
{
  "sessions": [
    {
      "session_id": "session_123",
      "created_at": "2024-01-01 10:00:00",
      "last_activity": "2024-01-01 10:30:00",
      "message_count": 10
    }
  ]
}
```

### GET /api/session/:sessionId

**Response**:
```json
{
  "sessionId": "session_123",
  "history": [
    {
      "role": "user",
      "content": "How do I reset my password?",
      "timestamp": "2024-01-01 10:00:00"
    }
  ],
  "stats": {
    "message_count": 10,
    "total_tokens": 1500,
    "avg_similarity": 0.82
  }
}
```

**Error Handling**:
- 400: Invalid input
- 401: Invalid API key
- 429: Rate limit exceeded
- 500: Server error

## ğŸ›  Tech Stack

- **Backend**: Node.js + Express
- **LLM**: Google Gemini 1.5 Flash
- **Embeddings**: Google text-embedding-004
- **Vector Store**: In-memory (production: use Pinecone/Weaviate)
- **Frontend**: Vanilla JavaScript + HTML/CSS

## âœ¨ Features

- âœ… Real embedding-based retrieval (Google gemini-embedding-001)
- âœ… Cosine similarity search
- âœ… Similarity threshold filtering (0.7)
- âœ… Document chunking (300 words with 50-word overlap)
- âœ… Conversation history (SQLite persistence)
- âœ… Session management (SQLite)
- âœ… Token usage tracking
- âœ… Error handling (API failures, timeouts, rate limits)
- âœ… Fallback responses (when LLM unavailable)
- âœ… Loading states
- âœ… Responsive UI
- âœ… New chat functionality
- âœ… Multiple API endpoints
- âœ… Database persistence

## ğŸ“¸ Screenshots

![Chat Interface](screenshot.png)

## ğŸ¥ Demo Video

[Link to demo video]

## ğŸ“ Notes

- Vector store initializes on server start
- Sessions stored in memory (use Redis for production)
- Embeddings cached (no regeneration needed)
- Supports 10 sample documents covering common topics

## ğŸ”’ Security

- API key stored in environment variables
- Input validation on all endpoints
- Rate limiting handled by OpenAI
- No PII stored in logs

## ğŸš€ Production Considerations

- Replace in-memory storage with persistent vector DB
- Add authentication/authorization
- Implement request rate limiting
- Add monitoring and logging
- Use Redis for session management
- Deploy with PM2 or Docker
