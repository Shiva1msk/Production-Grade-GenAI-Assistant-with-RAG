
ğŸ¤– Production-Grade GenAI Assistant with RAG

A production-ready AI support assistant powered by Google Gemini 2.5 Flash and Retrieval-Augmented Generation (RAG).
The system delivers grounded, context-aware responses using semantic search and SQLite session persistence.

ğŸš€ Built with modern Gemini 2.5 models (2025)

âœ… System Status: FULLY OPERATIONAL

Backend: Node.js + Express âœ…

Database: SQLite âœ…

Embeddings: gemini-embedding-001 âœ…

Chat Model: gemini-2.5-flash âœ…

Vector Store: 10 chunks loaded âœ…

RAG Pipeline: Complete âœ…

Session Management: SQLite âœ…

ğŸ— Architecture

User â†’ Express API â†’ Query Embedding â†’ Similarity Search â†’ Context Retrieval â†’ Gemini 2.5 Flash â†’ Response

ğŸ”„ RAG Workflow
1ï¸âƒ£ Document Ingestion

Load documents from docs.json

Chunk content (300 words + overlap)

Generate embeddings using gemini-embedding-001

Store in vector database

2ï¸âƒ£ Query Processing

Convert user question â†’ embedding

Cosine similarity search

Retrieve top-K relevant chunks

Apply similarity threshold (0.7)

3ï¸âƒ£ Response Generation

Inject retrieved context into prompt

Include conversation history

Generate grounded response with Gemini 2.5 Flash

Low temperature (0.2) to reduce hallucinations

ğŸ“Š Embedding Strategy

Model: gemini-embedding-001

Dimensions: 3072

Chunk size: 300 words

Overlap: 50 words

Similarity: Cosine similarity

Threshold: 0.7

Retrieval: Top-3

ğŸ’¬ Grounding Prompt Strategy

The assistant is forced to:

âœ… Answer only from retrieved context

âœ… Refuse when info missing

âœ… Maintain conversation memory

âœ… Avoid hallucinations

Fallback message triggers when similarity is low.

ğŸš€ Quick Start
1. Install dependencies
npm install
2. Configure environment
cp .env.example .env

Add your key:

GEMINI_API_KEY=your_key_here
PORT=3000
3. Generate vector store
npm run ingest
4. Start server
npm start

Open:

http://localhost:3000
ğŸ“¡ API Endpoints
POST /api/chat
{
  "sessionId": "session_123",
  "message": "How can I reset my password?"
}
GET /api/health

Returns system status and model info.

ğŸ›  Tech Stack

Backend: Node.js + Express

LLM: Gemini 2.5 Flash

Embeddings: gemini-embedding-001

Vector Store: JSON (in-memory)

Database: SQLite

Frontend: Vanilla JS

ğŸ”’ Security

API keys stored in environment variables

.env excluded via .gitignore

API key restricted to Generative Language API

Input validation on all endpoints

ğŸš€ Production Improvements (Future)

Vector DB (Pinecone / Weaviate)

Redis session store

Streaming responses

Auth layer

Monitoring

â­ If you like this project, give it a star!
